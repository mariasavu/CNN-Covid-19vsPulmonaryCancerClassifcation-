{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariasavu/CNN-Covid-19vsPulmonaryCancerClassifcation-/blob/main/lastChance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEefIVqYGK_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4db703-2dba-481e-bfa2-520e37d7e95f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TTlDLB0bMFJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "!echo \"Loading data\"\n",
        "#!pip install gdown\n",
        "#trebuie verficat path ul\n",
        "!unzip '/content/drive/MyDrive/date_80_20.zip'\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras import optimizers\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "\n",
        "def save_model(model, history, name):\n",
        "    model.save(f'saved_models/{name}.h5')\n",
        "\n",
        "    with open(f'saved_models/{name}_history.pickle', mode='wb') as f:\n",
        "        pickle.dump(history.history, f)\n",
        "def plot_history(h):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
        "    ax = axes[0]\n",
        "    ax.plot(h.history['acc'], 'r', label='Training acc')\n",
        "    ax.plot(h.history['val_acc'], 'g', label='Validation acc')\n",
        "    ax.set_title('Training and validation accuracy')\n",
        "    ax.legend()\n",
        "    ax.grid()\n",
        "\n",
        "    ax = axes[1]\n",
        "    ax.plot(h.history['loss'], 'r', label='Training loss')\n",
        "    ax.plot(h.history['val_loss'], 'g', label='Validation loss')\n",
        "    ax.set_title('Training and validation loss')\n",
        "    ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcv1ymuqP14k",
        "outputId": "f4fcf78c-426c-46a8-b1f2-d10763ce74fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training malign images: 451\n",
            "training covid images: 440\n",
            "testing malign images: 106\n",
            "testing covid images: 107\n"
          ]
        }
      ],
      "source": [
        "root_path = '/content/date_80_20'\n",
        "train_dir =os.path.join(root_path, 'Train')\n",
        "test_dir = os.path.join(root_path, 'Test')\n",
        "\n",
        "train_mal_dir = os.path.join(train_dir, 'Cancer')\n",
        "train_covid_dir = os.path.join(train_dir, 'Covid')\n",
        "\n",
        "test_mal_dir = os.path.join(test_dir, 'Cancer')\n",
        "test_covid_dir = os.path.join(test_dir, 'Covid')\n",
        "\n",
        "\n",
        "print('training malign images:', len(os.listdir(train_mal_dir)))\n",
        "print('training covid images:', len(os.listdir(train_covid_dir)))\n",
        "print('testing malign images:', len(os.listdir(test_mal_dir)))\n",
        "print('testing covid images:', len(os.listdir(test_covid_dir)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZJHKygzcKHm"
      },
      "outputs": [],
      "source": [
        "#function to remove the files added by MacOS while adding files to archive as it may be interpreted as a diffrent class in our network\n",
        "def remove_ds_store(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file == '.DS_Store':\n",
        "                file_path = os.path.join(root, file)\n",
        "                os.remove(file_path)\n",
        "\n",
        "\n",
        "# Replace with the path to your directory\n",
        "remove_ds_store(train_dir)\n",
        "remove_ds_store(test_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJbwnw7ecPgz",
        "outputId": "3580091f-b882-4f03-9ea8-c7acdef7e574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 2\n",
            "Class names: ['Cancer', 'Covid']\n"
          ]
        }
      ],
      "source": [
        "#checking to see if the class names are correct\n",
        "class_names = os.listdir(test_dir)\n",
        "num_classes = len(class_names)\n",
        "\n",
        "print(\"Number of classes:\", num_classes)\n",
        "print(\"Class names:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah0bi0XYcQWL",
        "outputId": "9e88aad7-e9a4-4f11-ef08-3670276ca52f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 177 images belonging to 2 classes.\n",
            "valid_samples: 177\n",
            "Found 712 images belonging to 2 classes.\n",
            "train_samples: 712\n",
            "Found 211 images belonging to 2 classes.\n",
            "test_samples: 211\n",
            "Total number of images during training: 45568\n"
          ]
        }
      ],
      "source": [
        "#starting with data augumentation\n",
        "import numpy as np\n",
        "import random\n",
        "BATCH_SIZE = 64\n",
        "size = (120, 120)\n",
        "seed = random.randint(0, 9999)\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "# Create a flow from the directory for validation data - seed=42\n",
        "valid_gen = valid_datagen.flow_from_directory(train_dir, subset='validation',\n",
        "                                              shuffle=True, seed=seed,\n",
        "                                              target_size=size,\n",
        "                                              batch_size=BATCH_SIZE)\n",
        "print(f'valid_samples: {valid_gen.n}')\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "\n",
        "    rotation_range=30,  # Increased rotation range\n",
        "    width_shift_range=0.2,  # Increased shift range\n",
        "    height_shift_range=0.2,  # Increased shift range\n",
        "    validation_split=0.2,\n",
        "    zoom_range=0.2,  # Increased zoom range\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,  # Added vertical flip\n",
        "    shear_range=0.2,  # Added shear range\n",
        "    fill_mode='nearest',\n",
        "    brightness_range=(0.8, 1.2),  # Adjust brightness\n",
        "    channel_shift_range=20,  # Random channel shifts\n",
        "    # Add more augmentation techniques as desired\n",
        ")\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    seed=seed,\n",
        "    target_size=size,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "train_samples = train_generator.n\n",
        "print(f'train_samples: {train_samples}')\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=applications.vgg19.preprocess_input)\n",
        "\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    shuffle=False,\n",
        "    seed=seed,\n",
        "    target_size=size,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "test_samples = test_generator.n\n",
        "print(f'test_samples: {test_samples}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "total_train_images = train_samples * BATCH_SIZE\n",
        "print(\"Total number of images during training:\", total_train_images)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaA4dvkci8aD"
      },
      "outputs": [],
      "source": [
        "#def the model arhitecture using the ResNet base\n",
        "checkpoint_filepath = '/checkpoint'\n",
        "MODEL_FNAME='ResNet-80-20V2'\n",
        "epoci = 50\n",
        "conv_base = applications.ResNet50V2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=size + (3,)\n",
        ")\n",
        "conv_base.trainable = False\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(64, activation='relu', kernel_regularizer='l2'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(2, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
        "    metrics=['acc']\n",
        ")\n",
        "early_stop=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=4, verbose=1, restore_best_weights=True)\n",
        "rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.4,  patience=1, verbose=1)\n",
        "weight_m = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=valid_gen,\n",
        "    epochs=epoci,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop, rlronp],\n",
        "    validation_steps=None,  shuffle=True\n",
        ")\n",
        "performance=model.evaluate( test_generator,  steps=BATCH_SIZE, verbose=1 )[1] * 100\n",
        "save_model(model, hist, MODEL_FNAME) #    ax.grid()\n",
        "train_loss=min(hist.history['loss'])\n",
        "val_loss=min(hist.history['val_loss'])\n",
        "train_acc=max(hist.history['acc'])\n",
        "val_acc=max(hist.history['val_acc'])\n",
        "plot_history(hist)\n",
        "print('train_loss',train_loss)\n",
        "print('val_loss',val_loss)\n",
        "print('train_acc',train_acc*100)\n",
        "print('val_acc',val_acc*100)\n",
        "print('test_acc ', performance, ' %')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "js7beXb0qDw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a function that would plot the training process\n",
        "def plot_history(h):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
        "    ax = axes[0]\n",
        "    ax.plot(h.history['accuracy'], 'r', label='Training accuracy')\n",
        "    ax.plot(h.history['val_accuracy'], 'g', label='Validation accuracy')\n",
        "    ax.set_title('Training and validation accuracy')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.legend()\n",
        "\n",
        "    ax = axes[1]\n",
        "    ax.plot(h.history['loss'], 'r', label='Training loss')\n",
        "    ax.plot(h.history['val_loss'], 'g', label='Validation loss')\n",
        "    ax.set_title('Training and validation loss')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "y0LpzM9hmNC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH9d9xCFE26R",
        "outputId": "8b0a8e35-2e14-41b7-fe1e-6a7b763d3e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "model =VGG19(weights = \"imagenet\", include_top=False, input_shape = size + (3,))\n",
        "#model.layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras.layers import Activation, Dense, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "#Adding custom Layers\n",
        "x = model.output\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "predictions = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=model.input, outputs=predictions)\n",
        "y_pred = model.predict(X_test)\n",
        "confusion_matrix = sklearn.metrics.confusion_matrix(y_test, np.rint(y_pred))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GdGJt9oiYQJV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f6760c-2686-46fa-dec7-fd9f37d8939c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 120, 120, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 120, 120, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 120, 120, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 60, 60, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 60, 60, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 30, 30, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 30, 30, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 30, 30, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 30, 30, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, 30, 30, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 15, 15, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 15, 15, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512)               2359808   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,516,034\n",
            "Trainable params: 2,491,650\n",
            "Non-trainable params: 20,024,384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile the model\n",
        "opt =optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=4, verbose=1, restore_best_weights=True)\n",
        "rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.4,  patience=1, verbose=1)\n",
        "weight_m = model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=valid_gen,\n",
        "    epochs=epoci,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop, rlronp],\n",
        "    validation_steps=None,  shuffle=True\n",
        ")\n",
        "performance=model.evaluate( test_generator,  steps=BATCH_SIZE, verbose=1 )[1] * 100\n",
        "save_model(model, hist, MODEL_FNAME) #    ax.grid()\n",
        "train_loss=min(hist.history['loss'])\n",
        "val_loss=min(hist.history['val_loss'])\n",
        "train_acc=max(hist.history['accuracy'])\n",
        "val_acc=max(hist.history['val_acc'])\n",
        "plot_history(hist)\n",
        "print('train_loss',train_loss)\n",
        "print('val_loss',val_loss)\n",
        "print('train_acc',train_acc*100)\n",
        "print('val_acc',val_acc*100)\n",
        "print('test_acc ', performance, ' %')"
      ],
      "metadata": {
        "id": "R0QZC8nrZMDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.layers import Conv2D,MaxPooling2D\n",
        "from keras.layers import Activation, Dense, Flatten, Dropout\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "checkpoint_filepath = '/checkpoint'\n",
        "MODEL_FNAME='VGG-80-20VBATCH64SGD' # 4 are mai multe straturi\n",
        "epoci = 80\n",
        "model = VGG19(weights=\"imagenet\", include_top=False, input_shape=size + (3,))\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "x = model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "\n",
        "x = Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.03))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(2, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = Model(inputs=model.input, outputs=predictions)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pV4PEp3Zm3g",
        "outputId": "c0d0bb3c-462c-4c1c-98f6-daa0f7a6bb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 120, 120, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 120, 120, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 120, 120, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 60, 60, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 60, 60, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 30, 30, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 30, 30, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 30, 30, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 30, 30, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv4 (Conv2D)       (None, 30, 30, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 15, 15, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 15, 15, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv4 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv4 (Conv2D)       (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 256)               1179904   \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,238,978\n",
            "Trainable params: 1,213,826\n",
            "Non-trainable params: 20,025,152\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model with the VGG-19 base\n",
        "opt = optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=4, verbose=1, restore_best_weights=True)\n",
        "rlronp = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.4, patience=1, verbose=1)\n",
        "weight_m = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', mode='max', save_best_only=True)\n",
        "\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=valid_gen,\n",
        "    epochs=epoci,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stop, rlronp, weight_m],\n",
        "    validation_steps=len(valid_gen),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Evaluate on the test set\n",
        "steps = test_generator.samples // BATCH_SIZE\n",
        "performance = model.evaluate(test_generator, steps=steps, verbose=1)[1] * 100\n",
        "\n",
        "# Save the model and its history\n",
        "model.save(MODEL_FNAME + '.h5')\n",
        "with open(MODEL_FNAME + '_history.pkl', 'wb') as file:\n",
        "    pickle.dump(hist.history, file)\n",
        "\n",
        "# Print and display results\n",
        "train_loss = min(hist.history['loss'])\n",
        "val_loss = min(hist.history['val_loss'])\n",
        "train_acc = max(hist.history['accuracy'])\n",
        "val_acc = max(hist.history['val_accuracy'])\n",
        "plot_history(hist)\n",
        "\n",
        "print('train_loss:', train_loss)\n",
        "print('val_loss:', val_loss)\n",
        "print('train_acc:', train_acc * 100)\n",
        "print('val_acc:', val_acc * 100)\n",
        "print('test_acc:', performance, '%')\n"
      ],
      "metadata": {
        "id": "BbW3uUZBiJy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xARF2MaMECIK"
      },
      "outputs": [],
      "source": [
        "# to store the modeles directly to the drive\n",
        "\n",
        "!cp -r \"/content/saved_models\" \"/content/drive/MyDrive\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/VGG-80-20VBATCH64_history.pkl\" \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "id": "-fVC8BrjYMkk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNqoJE5qVISbY//IPLdkJm1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}